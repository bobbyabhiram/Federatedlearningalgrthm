{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FEMNIST_Results.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b415ab1c60c4806a97464206543c4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57a0b6f64b204e0aa4fb339d7be32b0e",
              "IPY_MODEL_028a88f825614b49a90f6d2719ac3fb4"
            ],
            "layout": "IPY_MODEL_4ad5f22265ca4864bc7c9df2f62c9c31"
          }
        },
        "57a0b6f64b204e0aa4fb339d7be32b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Dl Completed...: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6612419717dd4a5f8afb4295d14c76f1",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_384f6bd0d19845e59a4cbf5ff076ab16",
            "value": 4
          }
        },
        "028a88f825614b49a90f6d2719ac3fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2452077c599148cbab8a1c8982377dc8",
            "placeholder": "​",
            "style": "IPY_MODEL_9ce9d455b2824243a2d9dbe60096deb8",
            "value": " 4/4 [00:07&lt;00:00,  1.89s/ file]"
          }
        },
        "4ad5f22265ca4864bc7c9df2f62c9c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6612419717dd4a5f8afb4295d14c76f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384f6bd0d19845e59a4cbf5ff076ab16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2452077c599148cbab8a1c8982377dc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ce9d455b2824243a2d9dbe60096deb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7b415ab1c60c4806a97464206543c4b8",
            "57a0b6f64b204e0aa4fb339d7be32b0e",
            "028a88f825614b49a90f6d2719ac3fb4",
            "4ad5f22265ca4864bc7c9df2f62c9c31",
            "6612419717dd4a5f8afb4295d14c76f1",
            "384f6bd0d19845e59a4cbf5ff076ab16",
            "2452077c599148cbab8a1c8982377dc8",
            "9ce9d455b2824243a2d9dbe60096deb8"
          ]
        },
        "id": "JbTH-z5l0luA",
        "outputId": "d55311db-671c-447c-bec2-49a1ef8374f5"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "from tqdm import trange\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import tensorflow_datasets\n",
        "\n",
        "\n",
        "mnist = tensorflow_datasets.load('mnist')\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "NUM_USERS = 100\n",
        "NUM_LABELS = 2\n",
        "\n",
        "# Setup directory for train/test data\n",
        "train_path = './data/train/fashion_train.json'\n",
        "test_path = './data/test/fashion_test.json'\n",
        "dir_path = os.path.dirname(train_path)\n",
        "if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "dir_path = os.path.dirname(test_path)\n",
        "if not os.path.exists(dir_path):\n",
        "    os.makedirs(dir_path)\n",
        "import tensorflow as tf\n",
        "# Import Fashion MNIST\n",
        "fashion_data = fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "print(\"hello\")\n",
        "#fashion_full = list(zip(fashion_data.train, fashion_data.test))\n",
        "print(type(fashion_data))\n",
        "mnist_data_image = []\n",
        "mnist_data_lable = []\n",
        "mnist_data_image.extend(train_images)\n",
        "mnist_data_image.extend(test_images)\n",
        "mnist_data_lable.extend(train_labels)\n",
        "mnist_data_lable.extend(test_labels)\n",
        "\n",
        "mu = np.mean(mnist_data_image)\n",
        "sigma = np.std(mnist_data_image)\n",
        "nom_Fashion_data = (mnist_data_image - mu)/(sigma+0.001)\n",
        "nom_Fashion_lable = np.array(mnist_data_lable)\n",
        "mnist_data = []\n",
        "for i in trange(10):\n",
        "    idx = nom_Fashion_lable == i\n",
        "    mnist_data.append(nom_Fashion_data[idx])\n",
        "\n",
        "print([len(v) for v in mnist_data])\n",
        "\n",
        "\n",
        "###### CREATE USER DATA SPLIT #######\n",
        "# Assign 100 samples to each user\n",
        "X = [[] for _ in range(NUM_USERS)]\n",
        "y = [[] for _ in range(NUM_USERS)]\n",
        "idx = np.zeros(10, dtype=np.int64)\n",
        "for user in range(NUM_USERS):\n",
        "    for j in range(NUM_LABELS):  # 3 labels for each users\n",
        "        # l = (2*user+j)%10\n",
        "        l = (user + j) % 10\n",
        "        print(\"L:\", l)\n",
        "        X[user] += mnist_data[l][idx[l]:idx[l]+10].tolist()\n",
        "        y[user] += (l*np.ones(10)).tolist()\n",
        "        idx[l] += 10\n",
        "\n",
        "print(\"IDX1:\", idx)  # counting samples for each labels\n",
        "\n",
        "# Assign remaining sample by power law\n",
        "user = 0\n",
        "props = np.random.lognormal(\n",
        "    0, 2., (10, NUM_USERS, NUM_LABELS))  # last 5 is 5 labels\n",
        "# print(\"here:\",props/np.sum(props,(1,2), keepdims=True))\n",
        "props = np.array([[[len(v)-100]] for v in mnist_data]) * \\\n",
        "    props/np.sum(props, (1, 2), keepdims=True)\n",
        "#idx = 1000*np.ones(10, dtype=np.int64)\n",
        "# print(\"here2:\",props)\n",
        "for user in trange(NUM_USERS):\n",
        "    for j in range(NUM_LABELS):  # 4 labels for each users\n",
        "        # l = (2*user+j)%10\n",
        "        l = (user + j) % 10\n",
        "        #num_samples = int(props[l,user//int(NUM_USERS/10),j]) *10\n",
        "        num_samples = int(props[l, user, j]) * NUM_USERS * 2\n",
        "        #num_samples = min(num_samples,200)\n",
        "        # print(num_samples)\n",
        "        if idx[l] + num_samples < len(mnist_data[l]):\n",
        "            X[user] += mnist_data[l][idx[l]:idx[l]+num_samples].tolist()\n",
        "            y[user] += (l*np.ones(num_samples)).tolist()\n",
        "            idx[l] += num_samples\n",
        "\n",
        "print(\"IDX2:\", idx)  # counting samples for each labels\n",
        "# Create data structure\n",
        "train_data = {'users': [], 'user_data': {}, 'num_samples': []}\n",
        "test_data = {'users': [], 'user_data': {}, 'num_samples': []}\n",
        "\n",
        "# Setup 5 users\n",
        "# for i in trange(5, ncols=120):\n",
        "for i in range(NUM_USERS):\n",
        "    uname = 'f_{0:05d}'.format(i)\n",
        "\n",
        "    combined = list(zip(X[i], y[i]))\n",
        "    random.shuffle(combined)\n",
        "    X[i][:], y[i][:] = zip(*combined)\n",
        "    num_samples = len(X[i])\n",
        "    train_len = int(0.75*num_samples)\n",
        "    test_len = num_samples - train_len\n",
        "\n",
        "    train_data['users'].append(uname)\n",
        "    train_data['user_data'][uname] = {\n",
        "        'x': X[i][:train_len], 'y': y[i][:train_len]}\n",
        "    train_data['num_samples'].append(train_len)\n",
        "    test_data['users'].append(uname)\n",
        "    test_data['user_data'][uname] = {\n",
        "        'x': X[i][train_len:], 'y': y[i][train_len:]}\n",
        "    test_data['num_samples'].append(test_len)\n",
        "\n",
        "print(\"Num_samples:\", train_data['num_samples'])\n",
        "print(\"Total_samples:\", sum(train_data['num_samples']))\n",
        "\n",
        "with open(train_path, 'w') as outfile:\n",
        "    json.dump(train_data, outfile)\n",
        "with open(test_path, 'w') as outfile:\n",
        "    json.dump(test_data, outfile)\n",
        "\n",
        "print(\"Finish Generating Samples\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead pass\n",
            "`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b415ab1c60c4806a97464206543c4b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Dl Completed...', max=4.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "hello\n",
            "<class 'module'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 86.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[7000, 7000, 7000, 7000, 7000, 7000, 7000, 7000, 7000, 7000]\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4/100 [00:00<00:13,  7.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "L: 0\n",
            "L: 1\n",
            "L: 1\n",
            "L: 2\n",
            "L: 2\n",
            "L: 3\n",
            "L: 3\n",
            "L: 4\n",
            "L: 4\n",
            "L: 5\n",
            "L: 5\n",
            "L: 6\n",
            "L: 6\n",
            "L: 7\n",
            "L: 7\n",
            "L: 8\n",
            "L: 8\n",
            "L: 9\n",
            "L: 9\n",
            "L: 0\n",
            "IDX1: [200 200 200 200 200 200 200 200 200 200]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:05<00:00, 17.62it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "IDX2: [6600 6600 6800 6000 6600 6800 6600 6400 6800 6400]\n",
            "Num_samples: [615, 1965, 315, 1365, 915, 1815, 1365, 465, 1365, 15, 465, 1665, 765, 1515, 165, 15, 2265, 615, 615, 915, 465, 915, 465, 165, 15, 1515, 15, 765, 2415, 615, 465, 315, 465, 2115, 1365, 15, 315, 615, 315, 765, 15, 465, 15, 165, 1665, 15, 15, 3015, 165, 1515, 15, 915, 465, 15, 1665, 165, 15, 15, 15, 15, 1665, 15, 165, 465, 1365, 165, 15, 315, 165, 15, 15, 165, 15, 15, 15, 15, 15, 15, 15, 165, 165, 15, 15, 15, 15, 15, 15, 15, 15, 165, 15, 15, 15, 15, 165, 15, 15, 15, 15, 15]\n",
            "Total_samples: 49200\n",
            "Finish Generating Samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "ksbxzWtfq0CM",
        "outputId": "dc4aa52f-d275-4d73-c2bb-b1cd56ada917"
      },
      "source": [
        "! pip install Flearn \n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import importlib\n",
        "import random\n",
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "import Flearn\n",
        "tf.disable_v2_behavior() \n",
        "from flearn.utils.plot_utils import *\n",
        "from flearn.utils.model_utils import read_data\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# GLOBAL PARAMETERS\n",
        "OPTIMIZERS = ['fedsgd', 'fedfedl']\n",
        "\n",
        "DATASETS = ['nist', 'mnist', 'fashion_mnist']  # NIST is EMNIST in the paper\n",
        "\n",
        "MODEL_PARAMS = {\n",
        "    'sent140.bag_dnn': (2,),  # num_classes\n",
        "    'sent140.stacked_lstm': (25, 2, 100),  # seq_len, num_classes, num_hidden\n",
        "    # seq_len, num_classes, num_hidden\n",
        "    'sent140.stacked_lstm_no_embeddings': (25, 2, 100),\n",
        "    # num_classes, should be changed to 62 when using EMNIST\n",
        "    'nist.mclr': (62,),\n",
        "    'nist.cnn': (62,),\n",
        "    'mnist.mclr': (10,),  # num_classes\n",
        "    'mnist.cnn': (10,),  # num_classes\n",
        "    'fashion_mnist.mclr': (10,),\n",
        "    'fashion_mnist.cnn': (10,),\n",
        "    'shakespeare.stacked_lstm': (80, 80, 256),  # seq_len, emb_dim, num_hidden\n",
        "    'synthetic.mclr': (10, )  # num_classes\n",
        "}\n",
        "\n",
        "\n",
        "def read_options(num_users=5, loc_ep=10, Numb_Glob_Iters=100, lamb=0, learning_rate=0.01, hyper_learning_rate = 0.01, alg='fedprox', weight=True, batch_size=0, times = 10, rho = 0, dataset=\"mnist\"):\n",
        "    ''' Parse command line arguments or load defaults '''\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--optimizer',\n",
        "                        help='name of optimizer;',\n",
        "                        type=str,\n",
        "                        choices=OPTIMIZERS,\n",
        "                        default=alg)  # fedavg, fedprox\n",
        "    parser.add_argument('--dataset',\n",
        "                        help='name of dataset;',\n",
        "                        type=str,\n",
        "                        choices=DATASETS,\n",
        "                        default=dataset)\n",
        "    parser.add_argument('--model',\n",
        "                        help='name of model;',\n",
        "                        type=str,\n",
        "                        default='mclr.py')  # 'stacked_lstm.py'\n",
        "    parser.add_argument('--num_rounds',\n",
        "                        help='number of rounds to simulate;',\n",
        "                        type=int,\n",
        "                        default=Numb_Glob_Iters)\n",
        "    parser.add_argument('--eval_every',\n",
        "                        help='evaluate every __ rounds;',\n",
        "                        type=int,\n",
        "                        default=1)\n",
        "    parser.add_argument('--clients_per_round',\n",
        "                        help='number of clients trained per round;',\n",
        "                        type=int,\n",
        "                        default=num_users)\n",
        "    parser.add_argument('--batch_size',\n",
        "                        help='batch size when clients train on data;',\n",
        "                        type=int,\n",
        "                        default=batch_size\n",
        "                        )  # 0 is full dataset\n",
        "    parser.add_argument('--num_epochs',\n",
        "                        help='number of epochs when clients train on data;',\n",
        "                        type=int,\n",
        "                        default=loc_ep)\n",
        "    parser.add_argument('--learning_rate',\n",
        "                        help='learning rate for inner solver;',\n",
        "                        type=float,\n",
        "                        default=learning_rate)  # 0.003\n",
        "    parser.add_argument('--hyper_learning_rate',\n",
        "                        help='learning rate for inner solver;',\n",
        "                        type=float,\n",
        "                        default=hyper_learning_rate)  # 0.003\n",
        "    parser.add_argument('--mu',\n",
        "                        help='constant for prox;',\n",
        "                        type=float,\n",
        "                        default=0.)  # 0.01\n",
        "    parser.add_argument('--seed',\n",
        "                        help='seed for randomness;',\n",
        "                        type=int,\n",
        "                        default=0)\n",
        "    parser.add_argument('--weight',\n",
        "                        help='enable weight value;',\n",
        "                        type=int,\n",
        "                        default=weight)\n",
        "    parser.add_argument('--lamb',\n",
        "                        help='Penalty value for proximal term;',\n",
        "                        type=int,\n",
        "                        default=lamb)\n",
        "    parser.add_argument('--times',\n",
        "                        help='Number of running time;',\n",
        "                        type=int,\n",
        "                        default=1)\n",
        "    parser.add_argument('--rho',\n",
        "                        help='Condition number only for synthetic data;',\n",
        "                        type=float,\n",
        "                        default=rho)\n",
        "            \n",
        "    try:\n",
        "        parsed = vars(parser.parse_args())\n",
        "    except IOError as msg:\n",
        "        parser.error(str(msg))\n",
        "\n",
        "    # Set seeds\n",
        "    random.seed(1 + parsed['seed'])\n",
        "    np.random.seed(12 + parsed['seed'])\n",
        "    tf.set_random_seed(123 + parsed['seed'])\n",
        "\n",
        "    # load selected model\n",
        "    # all synthetic datasets use the same model\n",
        "    if parsed['dataset'].startswith(\"synthetic\"):\n",
        "        model_path = '%s.%s.%s.%s' % (\n",
        "            'flearn', 'models', 'synthetic', parsed['model'])\n",
        "    else:\n",
        "        model_path = '%s.%s.%s.%s' % (\n",
        "            'flearn', 'models', parsed['dataset'], parsed['model'])\n",
        "\n",
        "    # mod = importlib.import_module(model_path)\n",
        "    import flearn.models.mnist.mclr as mclr\n",
        "    mod = mclr\n",
        "    learner = getattr(mod, 'Model')\n",
        "\n",
        "    # load selected trainer\n",
        "    opt_path = 'flearn.trainers.%s' % parsed['optimizer']\n",
        "    mod = importlib.import_module(opt_path)\n",
        "    optimizer = getattr(mod, 'Server')\n",
        "\n",
        "    # add selected model parameter\n",
        "    parsed['model_params'] = MODEL_PARAMS['.'.join(\n",
        "        model_path.split('.')[2:-1])]\n",
        "    # parsed['model_params'] = MODEL_PARAMS['mnist.mclr']\n",
        "\n",
        "    # print and return\n",
        "    maxLen = max([len(ii) for ii in parsed.keys()])\n",
        "    fmtString = '\\t%' + str(maxLen) + 's : %s'\n",
        "    print('Arguments:')\n",
        "    for keyPair in sorted(parsed.items()):\n",
        "        print(fmtString % keyPair)\n",
        "\n",
        "    return parsed, learner, optimizer\n",
        "\n",
        "\n",
        "def main(num_users=5, loc_ep=10, Numb_Glob_Iters=100, lamb=0, learning_rate=0.01,hyper_learning_rate= 0.01, alg='fedprox', weight=True, batch_size=0, times =10, rho = 0, dataset=\"mnist\"):\n",
        "    # suppress tf warnings\n",
        "    tf.logging.set_verbosity(tf.logging.WARN)\n",
        "\n",
        "    # parse command line arguments\n",
        "    options, learner, optimizer = read_options(\n",
        "        num_users, loc_ep, Numb_Glob_Iters, lamb, learning_rate,hyper_learning_rate, alg, weight, batch_size, times, rho, dataset)\n",
        "\n",
        "    # read data\n",
        "    train_path = os.path.join('data', options['dataset'], 'data', 'train')\n",
        "    test_path = os.path.join('data', options['dataset'], 'data', 'test')\n",
        "    dataset = read_data(train_path, test_path)\n",
        "\n",
        "       # call appropriate trainer\n",
        "    for i in range(times):\n",
        "        # Set seeds\n",
        "        random.seed(1 + i)\n",
        "        np.random.seed(12 + i)\n",
        "        tf.set_random_seed(123 + i)\n",
        "        print('......time for runing......',i)\n",
        "        t = optimizer(options, learner, dataset)\n",
        "        t.train(i)\n",
        "    average_data(num_users=num_users, loc_ep1=loc_ep, Numb_Glob_Iters=Numb_Glob_Iters, lamb=lamb, learning_rate=learning_rate, hyper_learning_rate = hyper_learning_rate, algorithms=alg, batch_size=batch_size, dataset=dataset, rho = rho, times = times)\n",
        "\n",
        "\n",
        "\n",
        "# if _name_ == '_main_':\n",
        "#     algorithms_list = [\"fedfedl\", \"fedsgd\",\n",
        "#                        \"fedfedl\", \"fedsgd\", \n",
        "#                        \"fedfedl\", \"fedsgd\"]\n",
        "#     lamb_value = [0, 0, 0, 0, 0, 0]\n",
        "#     learning_rate = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
        "#     local_ep = [10,10, 20, 20, 50, 50]\n",
        "#     batch_size = [20,20, 20, 20, 20, 20]\n",
        "#     DATA_SET = \"nist\"\n",
        "#     number_users = 30\n",
        "\n",
        "#     for i in range(len(algorithms_list)):\n",
        "#         main(num_users=number_users, loc_ep=local_ep[i], Numb_Glob_Iters=1000, lamb=lamb_value[i],\n",
        "#              learning_rate=learning_rate[i], alg=algorithms_list[i], batch_size=batch_size[i], rho = rho[i], dataset=DATA_SET)\n",
        "\n",
        "#     plot_summary_three_figures(num_users=number_users, loc_ep1=local_ep, Numb_Glob_Iters=800, lamb=lamb_value,\n",
        "#                             learning_rate=learning_rate, algorithms_list=algorithms_list, batch_size=batch_size, rho = rho[i], dataset=DATA_SET)\n",
        "#     print(\"-- FINISH -- :\",)\n",
        "\n",
        "\n",
        "if _name_ == '_main_':\n",
        "    \n",
        "    algorithms_list =  [\"fedfedl\",\"fedsgd\",\"fedfedl\", \"fedfedl\",\"fedsgd\",\"fedfedl\", \"fedfedl\",\"fedsgd\",\"fedfedl\"]\n",
        "    rho = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    lamb_value = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "    learning_rate = [0.003, 0.003, 0.015, 0.003, 0.003, 0.015, 0.003, 0.003, 0.015]\n",
        "    hyper_learning_rate = [0.2, 0, 0.5, 0.2, 0, 0.5, 0.2, 0, 0.5]\n",
        "    local_ep = [10, 10, 10, 20, 20, 20, 40, 40, 40]\n",
        "    batch_size = [20, 20, 0, 20, 20, 0, 20, 20, 0]\n",
        "    DATA_SET = \"nist\"\n",
        "    number_users = 10\n",
        "    \n",
        "    # for i in range(len(algorithms_list)):\n",
        "    #     main(num_users=number_users, loc_ep=local_ep[i], Numb_Glob_Iters=800, lamb=lamb_value[i],\n",
        "    #          learning_rate=learning_rate[i],hyper_learning_rate=hyper_learning_rate[i],  alg=algorithms_list[i], batch_size=batch_size[i], rho = rho[i], dataset=DATA_SET)\n",
        "\n",
        "    plot_summary_nist(num_users=number_users, loc_ep1=local_ep, Numb_Glob_Iters=800, lamb=lamb_value,\n",
        "                               learning_rate=learning_rate, hyper_learning_rate = hyper_learning_rate, algorithms_list=algorithms_list, batch_size=batch_size, rho = rho, dataset=DATA_SET)\n",
        "    print(\"-- FINISH -- :\",)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Flearn in /usr/local/lib/python3.7/dist-packages (1.3.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.7/dist-packages/Flearn.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    print(each_item)\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    }
  ]
}